{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n",
      "WARNING:tensorflow:From C:\\Users\\thanusha\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-1-ce5f7656f603>:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-1-ce5f7656f603>:177: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  validation_loss: 2.302511215209961, validation_accuracy: 0.10379999876022339\n",
      "Epoch  2, CIFAR-10 Batch 1:  validation_loss: 2.1308491230010986, validation_accuracy: 0.17139999568462372\n",
      "Epoch  3, CIFAR-10 Batch 1:  validation_loss: 2.063812494277954, validation_accuracy: 0.21080000698566437\n",
      "Epoch  4, CIFAR-10 Batch 1:  validation_loss: 1.9698753356933594, validation_accuracy: 0.22120000422000885\n",
      "Epoch  5, CIFAR-10 Batch 1:  validation_loss: 1.8488374948501587, validation_accuracy: 0.28540000319480896\n",
      "Epoch  6, CIFAR-10 Batch 1:  validation_loss: 1.7409261465072632, validation_accuracy: 0.32120001316070557\n",
      "Epoch  7, CIFAR-10 Batch 1:  validation_loss: 1.6966726779937744, validation_accuracy: 0.33180001378059387\n",
      "Epoch  8, CIFAR-10 Batch 1:  validation_loss: 1.6704511642456055, validation_accuracy: 0.3529999852180481\n",
      "Epoch  9, CIFAR-10 Batch 1:  validation_loss: 1.5670660734176636, validation_accuracy: 0.40799999237060547\n",
      "Epoch 10, CIFAR-10 Batch 1:  validation_loss: 1.5489771366119385, validation_accuracy: 0.4180000126361847\n",
      "Epoch 11, CIFAR-10 Batch 1:  validation_loss: 1.547176718711853, validation_accuracy: 0.41659998893737793\n",
      "Epoch 12, CIFAR-10 Batch 1:  validation_loss: 1.551956295967102, validation_accuracy: 0.4327999949455261\n",
      "Epoch 13, CIFAR-10 Batch 1:  validation_loss: 1.5764834880828857, validation_accuracy: 0.4320000112056732\n",
      "Epoch 14, CIFAR-10 Batch 1:  validation_loss: 1.570829153060913, validation_accuracy: 0.4611999988555908\n",
      "Epoch 15, CIFAR-10 Batch 1:  validation_loss: 1.6832590103149414, validation_accuracy: 0.45980000495910645\n",
      "Epoch 16, CIFAR-10 Batch 1:  validation_loss: 1.6998181343078613, validation_accuracy: 0.46320000290870667\n",
      "Epoch 17, CIFAR-10 Batch 1:  validation_loss: 1.8425109386444092, validation_accuracy: 0.45419999957084656\n",
      "Epoch 18, CIFAR-10 Batch 1:  validation_loss: 1.8333797454833984, validation_accuracy: 0.4535999894142151\n",
      "Epoch 19, CIFAR-10 Batch 1:  validation_loss: 1.9914156198501587, validation_accuracy: 0.4392000138759613\n",
      "Epoch 20, CIFAR-10 Batch 1:  validation_loss: 1.9710607528686523, validation_accuracy: 0.4422000050544739\n",
      "Epoch 21, CIFAR-10 Batch 1:  validation_loss: 2.1873021125793457, validation_accuracy: 0.45019999146461487\n",
      "Epoch 22, CIFAR-10 Batch 1:  validation_loss: 2.456043243408203, validation_accuracy: 0.4230000078678131\n",
      "Epoch 23, CIFAR-10 Batch 1:  validation_loss: 2.4524612426757812, validation_accuracy: 0.4472000002861023\n",
      "Epoch 24, CIFAR-10 Batch 1:  validation_loss: 2.48787784576416, validation_accuracy: 0.4480000138282776\n",
      "Epoch 25, CIFAR-10 Batch 1:  validation_loss: 2.399245262145996, validation_accuracy: 0.44359999895095825\n",
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  validation_loss: 2.2618749141693115, validation_accuracy: 0.15919999778270721\n",
      "Epoch  1, CIFAR-10 Batch 2:  validation_loss: 2.0205650329589844, validation_accuracy: 0.18559999763965607\n",
      "Epoch  1, CIFAR-10 Batch 3:  validation_loss: 1.9511137008666992, validation_accuracy: 0.18459999561309814\n",
      "Epoch  1, CIFAR-10 Batch 4:  validation_loss: 1.8713699579238892, validation_accuracy: 0.22599999606609344\n",
      "Epoch  1, CIFAR-10 Batch 5:  validation_loss: 1.8127108812332153, validation_accuracy: 0.26899999380111694\n",
      "Epoch  2, CIFAR-10 Batch 1:  validation_loss: 1.7944889068603516, validation_accuracy: 0.3093999922275543\n",
      "Epoch  2, CIFAR-10 Batch 2:  validation_loss: 1.7675142288208008, validation_accuracy: 0.2939999997615814\n",
      "Epoch  2, CIFAR-10 Batch 3:  validation_loss: 1.6485861539840698, validation_accuracy: 0.34439998865127563\n",
      "Epoch  2, CIFAR-10 Batch 4:  validation_loss: 1.586374044418335, validation_accuracy: 0.37860000133514404\n",
      "Epoch  2, CIFAR-10 Batch 5:  validation_loss: 1.5536258220672607, validation_accuracy: 0.4106000065803528\n",
      "Epoch  3, CIFAR-10 Batch 1:  validation_loss: 1.4945873022079468, validation_accuracy: 0.436599999666214\n",
      "Epoch  3, CIFAR-10 Batch 2:  validation_loss: 1.4655230045318604, validation_accuracy: 0.4546000063419342\n",
      "Epoch  3, CIFAR-10 Batch 3:  validation_loss: 1.4064048528671265, validation_accuracy: 0.4832000136375427\n",
      "Epoch  3, CIFAR-10 Batch 4:  validation_loss: 1.3660744428634644, validation_accuracy: 0.49380001425743103\n",
      "Epoch  3, CIFAR-10 Batch 5:  validation_loss: 1.337510108947754, validation_accuracy: 0.5116000175476074\n",
      "Epoch  4, CIFAR-10 Batch 1:  validation_loss: 1.3652838468551636, validation_accuracy: 0.5103999972343445\n",
      "Epoch  4, CIFAR-10 Batch 2:  validation_loss: 1.3218234777450562, validation_accuracy: 0.5202000141143799\n",
      "Epoch  4, CIFAR-10 Batch 3:  validation_loss: 1.2590186595916748, validation_accuracy: 0.5454000234603882\n",
      "Epoch  4, CIFAR-10 Batch 4:  validation_loss: 1.2395727634429932, validation_accuracy: 0.5537999868392944\n",
      "Epoch  4, CIFAR-10 Batch 5:  validation_loss: 1.2262290716171265, validation_accuracy: 0.5594000220298767\n",
      "Epoch  5, CIFAR-10 Batch 1:  validation_loss: 1.2924187183380127, validation_accuracy: 0.5361999869346619\n",
      "Epoch  5, CIFAR-10 Batch 2:  validation_loss: 1.1924412250518799, validation_accuracy: 0.573199987411499\n",
      "Epoch  5, CIFAR-10 Batch 3:  validation_loss: 1.2109861373901367, validation_accuracy: 0.5669999718666077\n",
      "Epoch  5, CIFAR-10 Batch 4:  validation_loss: 1.1892318725585938, validation_accuracy: 0.574999988079071\n",
      "Epoch  5, CIFAR-10 Batch 5:  validation_loss: 1.2887803316116333, validation_accuracy: 0.5428000092506409\n",
      "Epoch  6, CIFAR-10 Batch 1:  validation_loss: 1.2155253887176514, validation_accuracy: 0.5789999961853027\n",
      "Epoch  6, CIFAR-10 Batch 2:  validation_loss: 1.1646158695220947, validation_accuracy: 0.5848000049591064\n",
      "Epoch  6, CIFAR-10 Batch 3:  validation_loss: 1.1575978994369507, validation_accuracy: 0.5889999866485596\n",
      "Epoch  6, CIFAR-10 Batch 4:  validation_loss: 1.1770961284637451, validation_accuracy: 0.58160001039505\n",
      "Epoch  6, CIFAR-10 Batch 5:  validation_loss: 1.2356277704238892, validation_accuracy: 0.5667999982833862\n",
      "Epoch  7, CIFAR-10 Batch 1:  validation_loss: 1.2135707139968872, validation_accuracy: 0.5835999846458435\n",
      "Epoch  7, CIFAR-10 Batch 2:  validation_loss: 1.159028172492981, validation_accuracy: 0.6025999784469604\n",
      "Epoch  7, CIFAR-10 Batch 3:  validation_loss: 1.1551557779312134, validation_accuracy: 0.5978000164031982\n",
      "Epoch  7, CIFAR-10 Batch 4:  validation_loss: 1.213266372680664, validation_accuracy: 0.5756000280380249\n",
      "Epoch  7, CIFAR-10 Batch 5:  validation_loss: 1.1770426034927368, validation_accuracy: 0.5879999995231628\n",
      "Epoch  8, CIFAR-10 Batch 1:  validation_loss: 1.1736074686050415, validation_accuracy: 0.6086000204086304\n",
      "Epoch  8, CIFAR-10 Batch 2:  validation_loss: 1.1579967737197876, validation_accuracy: 0.6021999716758728\n",
      "Epoch  8, CIFAR-10 Batch 3:  validation_loss: 1.153611183166504, validation_accuracy: 0.6122000217437744\n",
      "Epoch  8, CIFAR-10 Batch 4:  validation_loss: 1.1373990774154663, validation_accuracy: 0.6075999736785889\n",
      "Epoch  8, CIFAR-10 Batch 5:  validation_loss: 1.1673603057861328, validation_accuracy: 0.6092000007629395\n",
      "Epoch  9, CIFAR-10 Batch 1:  validation_loss: 1.2307028770446777, validation_accuracy: 0.598800003528595\n",
      "Epoch  9, CIFAR-10 Batch 2:  validation_loss: 1.1880170106887817, validation_accuracy: 0.6064000129699707\n",
      "Epoch  9, CIFAR-10 Batch 3:  validation_loss: 1.2385945320129395, validation_accuracy: 0.5885999798774719\n",
      "Epoch  9, CIFAR-10 Batch 4:  validation_loss: 1.2101308107376099, validation_accuracy: 0.604200005531311\n",
      "Epoch  9, CIFAR-10 Batch 5:  validation_loss: 1.2593106031417847, validation_accuracy: 0.6025999784469604\n",
      "Epoch 10, CIFAR-10 Batch 1:  validation_loss: 1.2982820272445679, validation_accuracy: 0.5910000205039978\n",
      "Epoch 10, CIFAR-10 Batch 2:  validation_loss: 1.237318515777588, validation_accuracy: 0.603600025177002\n",
      "Epoch 10, CIFAR-10 Batch 3:  validation_loss: 1.3553096055984497, validation_accuracy: 0.5784000158309937\n",
      "Epoch 10, CIFAR-10 Batch 4:  validation_loss: 1.3023239374160767, validation_accuracy: 0.5946000218391418\n",
      "Epoch 10, CIFAR-10 Batch 5:  validation_loss: 1.3168593645095825, validation_accuracy: 0.5843999981880188\n",
      "Epoch 11, CIFAR-10 Batch 1:  validation_loss: 1.2774367332458496, validation_accuracy: 0.5910000205039978\n",
      "Epoch 11, CIFAR-10 Batch 2:  validation_loss: 1.241860270500183, validation_accuracy: 0.609000027179718\n",
      "Epoch 11, CIFAR-10 Batch 3:  validation_loss: 1.3475550413131714, validation_accuracy: 0.5813999772071838\n",
      "Epoch 11, CIFAR-10 Batch 4:  validation_loss: 1.2588119506835938, validation_accuracy: 0.6137999892234802\n",
      "Epoch 11, CIFAR-10 Batch 5:  validation_loss: 1.3273597955703735, validation_accuracy: 0.5960000157356262\n",
      "Epoch 12, CIFAR-10 Batch 1:  validation_loss: 1.3432012796401978, validation_accuracy: 0.5989999771118164\n",
      "Epoch 12, CIFAR-10 Batch 2:  validation_loss: 1.274318814277649, validation_accuracy: 0.6092000007629395\n",
      "Epoch 12, CIFAR-10 Batch 3:  validation_loss: 1.4145088195800781, validation_accuracy: 0.5979999899864197\n",
      "Epoch 12, CIFAR-10 Batch 4:  validation_loss: 1.2830106019973755, validation_accuracy: 0.6209999918937683\n",
      "Epoch 12, CIFAR-10 Batch 5:  validation_loss: 1.3085699081420898, validation_accuracy: 0.6141999959945679\n",
      "Epoch 13, CIFAR-10 Batch 1:  validation_loss: 1.3011773824691772, validation_accuracy: 0.6236000061035156\n",
      "Epoch 13, CIFAR-10 Batch 2:  validation_loss: 1.3813763856887817, validation_accuracy: 0.5946000218391418\n",
      "Epoch 13, CIFAR-10 Batch 3:  validation_loss: 1.3538691997528076, validation_accuracy: 0.6195999979972839\n",
      "Epoch 13, CIFAR-10 Batch 4:  validation_loss: 1.4323362112045288, validation_accuracy: 0.5943999886512756\n",
      "Epoch 13, CIFAR-10 Batch 5:  validation_loss: 1.4044109582901, validation_accuracy: 0.6230000257492065\n",
      "Epoch 14, CIFAR-10 Batch 1:  validation_loss: 1.3882436752319336, validation_accuracy: 0.6065999865531921\n",
      "Epoch 14, CIFAR-10 Batch 2:  validation_loss: 1.350488305091858, validation_accuracy: 0.6032000184059143\n",
      "Epoch 14, CIFAR-10 Batch 3:  validation_loss: 1.4698659181594849, validation_accuracy: 0.6122000217437744\n",
      "Epoch 14, CIFAR-10 Batch 4:  validation_loss: 1.5804152488708496, validation_accuracy: 0.5884000062942505\n",
      "Epoch 14, CIFAR-10 Batch 5:  validation_loss: 1.469050407409668, validation_accuracy: 0.6212000250816345\n",
      "Epoch 15, CIFAR-10 Batch 1:  validation_loss: 1.498963713645935, validation_accuracy: 0.6169999837875366\n",
      "Epoch 15, CIFAR-10 Batch 2:  validation_loss: 1.4588762521743774, validation_accuracy: 0.6074000000953674\n",
      "Epoch 15, CIFAR-10 Batch 3:  validation_loss: 1.6421082019805908, validation_accuracy: 0.5807999968528748\n",
      "Epoch 15, CIFAR-10 Batch 4:  validation_loss: 1.500610589981079, validation_accuracy: 0.6015999913215637\n",
      "Epoch 15, CIFAR-10 Batch 5:  validation_loss: 1.4755553007125854, validation_accuracy: 0.6255999803543091\n",
      "Epoch 16, CIFAR-10 Batch 1:  validation_loss: 1.6290078163146973, validation_accuracy: 0.6050000190734863\n",
      "Epoch 16, CIFAR-10 Batch 2:  validation_loss: 1.5595036745071411, validation_accuracy: 0.5997999906539917\n",
      "Epoch 16, CIFAR-10 Batch 3:  validation_loss: 1.6789976358413696, validation_accuracy: 0.5920000076293945\n",
      "Epoch 16, CIFAR-10 Batch 4:  validation_loss: 1.4595974683761597, validation_accuracy: 0.6230000257492065\n",
      "Epoch 16, CIFAR-10 Batch 5:  validation_loss: 1.5299102067947388, validation_accuracy: 0.6323999762535095\n",
      "Epoch 17, CIFAR-10 Batch 1:  validation_loss: 1.5344648361206055, validation_accuracy: 0.6200000047683716\n",
      "Epoch 17, CIFAR-10 Batch 2:  validation_loss: 1.768500804901123, validation_accuracy: 0.5947999954223633\n",
      "Epoch 17, CIFAR-10 Batch 3:  validation_loss: 1.827268123626709, validation_accuracy: 0.5914000272750854\n",
      "Epoch 17, CIFAR-10 Batch 4:  validation_loss: 1.6200042963027954, validation_accuracy: 0.6068000197410583\n",
      "Epoch 17, CIFAR-10 Batch 5:  validation_loss: 1.6541962623596191, validation_accuracy: 0.621999979019165\n",
      "Epoch 18, CIFAR-10 Batch 1:  validation_loss: 1.6423816680908203, validation_accuracy: 0.6233999729156494\n",
      "Epoch 18, CIFAR-10 Batch 2:  validation_loss: 1.7229958772659302, validation_accuracy: 0.6055999994277954\n",
      "Epoch 18, CIFAR-10 Batch 3:  validation_loss: 1.9571648836135864, validation_accuracy: 0.5860000252723694\n",
      "Epoch 18, CIFAR-10 Batch 4:  validation_loss: 1.5804706811904907, validation_accuracy: 0.6140000224113464\n",
      "Epoch 18, CIFAR-10 Batch 5:  validation_loss: 1.6212118864059448, validation_accuracy: 0.621399998664856\n",
      "Epoch 19, CIFAR-10 Batch 1:  validation_loss: 1.6942040920257568, validation_accuracy: 0.6075999736785889\n",
      "Epoch 19, CIFAR-10 Batch 2:  validation_loss: 1.792948842048645, validation_accuracy: 0.6136000156402588\n",
      "Epoch 19, CIFAR-10 Batch 3:  validation_loss: 1.8210852146148682, validation_accuracy: 0.6051999926567078\n",
      "Epoch 19, CIFAR-10 Batch 4:  validation_loss: 1.834091067314148, validation_accuracy: 0.6078000068664551\n",
      "Epoch 19, CIFAR-10 Batch 5:  validation_loss: 1.7710163593292236, validation_accuracy: 0.6179999709129333\n",
      "Epoch 20, CIFAR-10 Batch 1:  validation_loss: 1.773161768913269, validation_accuracy: 0.6104000210762024\n",
      "Epoch 20, CIFAR-10 Batch 2:  validation_loss: 1.8786073923110962, validation_accuracy: 0.6086000204086304\n",
      "Epoch 20, CIFAR-10 Batch 3:  validation_loss: 1.8407541513442993, validation_accuracy: 0.6037999987602234\n",
      "Epoch 20, CIFAR-10 Batch 4:  validation_loss: 1.9524755477905273, validation_accuracy: 0.6011999845504761\n",
      "Epoch 20, CIFAR-10 Batch 5:  validation_loss: 1.9051398038864136, validation_accuracy: 0.6087999939918518\n",
      "Epoch 21, CIFAR-10 Batch 1:  validation_loss: 1.6886203289031982, validation_accuracy: 0.6295999884605408\n",
      "Epoch 21, CIFAR-10 Batch 2:  validation_loss: 1.9126105308532715, validation_accuracy: 0.5985999703407288\n",
      "Epoch 21, CIFAR-10 Batch 3:  validation_loss: 1.9982120990753174, validation_accuracy: 0.6079999804496765\n",
      "Epoch 21, CIFAR-10 Batch 4:  validation_loss: 2.0951905250549316, validation_accuracy: 0.6010000109672546\n",
      "Epoch 21, CIFAR-10 Batch 5:  validation_loss: 2.1163933277130127, validation_accuracy: 0.6078000068664551\n",
      "Epoch 22, CIFAR-10 Batch 1:  validation_loss: 1.8592281341552734, validation_accuracy: 0.6177999973297119\n",
      "Epoch 22, CIFAR-10 Batch 2:  validation_loss: 1.701676607131958, validation_accuracy: 0.6140000224113464\n",
      "Epoch 22, CIFAR-10 Batch 3:  validation_loss: 1.9088964462280273, validation_accuracy: 0.6000000238418579\n",
      "Epoch 22, CIFAR-10 Batch 4:  validation_loss: 2.2799861431121826, validation_accuracy: 0.5992000102996826\n",
      "Epoch 22, CIFAR-10 Batch 5:  validation_loss: 2.086665630340576, validation_accuracy: 0.618399977684021\n",
      "Epoch 23, CIFAR-10 Batch 1:  validation_loss: 1.8614459037780762, validation_accuracy: 0.621399998664856\n",
      "Epoch 23, CIFAR-10 Batch 2:  validation_loss: 1.9302170276641846, validation_accuracy: 0.5943999886512756\n",
      "Epoch 23, CIFAR-10 Batch 3:  validation_loss: 2.2721738815307617, validation_accuracy: 0.5871999859809875\n",
      "Epoch 23, CIFAR-10 Batch 4:  validation_loss: 2.14355731010437, validation_accuracy: 0.623199999332428\n",
      "Epoch 23, CIFAR-10 Batch 5:  validation_loss: 2.166027069091797, validation_accuracy: 0.6057999730110168\n",
      "Epoch 24, CIFAR-10 Batch 1:  validation_loss: 1.9180959463119507, validation_accuracy: 0.6212000250816345\n",
      "Epoch 24, CIFAR-10 Batch 2:  validation_loss: 1.9968469142913818, validation_accuracy: 0.5920000076293945\n",
      "Epoch 24, CIFAR-10 Batch 3:  validation_loss: 2.4061214923858643, validation_accuracy: 0.5866000056266785\n",
      "Epoch 24, CIFAR-10 Batch 4:  validation_loss: 2.1289634704589844, validation_accuracy: 0.6240000128746033\n",
      "Epoch 24, CIFAR-10 Batch 5:  validation_loss: 2.0879571437835693, validation_accuracy: 0.6097999811172485\n",
      "Epoch 25, CIFAR-10 Batch 1:  validation_loss: 2.112119436264038, validation_accuracy: 0.6179999709129333\n",
      "Epoch 25, CIFAR-10 Batch 2:  validation_loss: 2.109992504119873, validation_accuracy: 0.599399983882904\n",
      "Epoch 25, CIFAR-10 Batch 3:  validation_loss: 2.2999343872070312, validation_accuracy: 0.6082000136375427\n",
      "Epoch 25, CIFAR-10 Batch 4:  validation_loss: 2.1862266063690186, validation_accuracy: 0.6128000020980835\n",
      "Epoch 25, CIFAR-10 Batch 5:  validation_loss: 2.390859603881836, validation_accuracy: 0.6019999980926514\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(range(10))\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)\n",
    "#normalize the data \n",
    "def normalize(x):\n",
    "    return x/255\n",
    "#one hot encoding\n",
    "def one_hot_encode(x):\n",
    "    #try using np.eye as well \n",
    "    return encoder.transform(x)\n",
    "\n",
    "\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)\n",
    "\n",
    "\n",
    "import pickle\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))\n",
    "\n",
    "#build the model\n",
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    imw, imh, cc = image_shape\n",
    "    x = tf.placeholder(tf.float32,\n",
    "    shape=[None, imw, imh, cc], name='x')\n",
    "    return x\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    y = tf.placeholder(tf.float32,\n",
    "    shape=[None, n_classes], name='y')\n",
    "    return y\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    k = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    return k\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#convolution and maxpool layers\n",
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    filter_size_width, filter_size_height = conv_ksize\n",
    "    color_channels = x_tensor.get_shape().as_list()[-1]\n",
    "    weight = tf.Variable(tf.truncated_normal(\n",
    "        [filter_size_width, filter_size_height, color_channels, conv_num_outputs], stddev=0.01))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "   \n",
    "    # Apply Convolution\n",
    "    # set the stride for batch and input_channels (i.e. the first and fourth element in the strides array) to be 1.\n",
    "    a, b = conv_strides\n",
    "    full_conv_strides = [1, a, b, 1]\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weight, strides=full_conv_strides, padding='SAME')\n",
    "    # Add bias\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    # Apply activation function\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    # Apply Max Pooling\n",
    "    a, b = pool_strides\n",
    "    full_pool_strides = [1, a, b, 1]\n",
    "    a, b = pool_ksize\n",
    "    full_pool_ksize = [1, a, b, 1]\n",
    "    conv_layer = tf.nn.max_pool(conv_layer,\n",
    "        ksize=full_pool_ksize,strides=full_pool_strides,\n",
    "        padding='SAME')\n",
    "    return conv_layer \n",
    "#flatten layer\n",
    "\n",
    "def flatten(x_tensor):\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    ns = np.prod(shape[1:])\n",
    "    flat = tf.reshape(x_tensor, [-1, ns])\n",
    "    return flat\n",
    "#fully connected layer\n",
    "def fully_conn(x_tensor, num_outputs):\n",
    "    weight = tf.Variable(tf.truncated_normal(\n",
    "        [x_tensor.get_shape().as_list()[-1] ,num_outputs], stddev=0.01))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    fc = tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "    fc = tf.nn.relu(fc)\n",
    "    return fc\n",
    "\n",
    "def output(x_tensor, num_outputs):\n",
    "    weight = tf.Variable(tf.truncated_normal(\n",
    "        [x_tensor.get_shape().as_list()[-1] , num_outputs], stddev=0.01))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    out = tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "    return out\n",
    "\n",
    "def conv_net(x, keep_prob):\n",
    "    \n",
    "    conv_num_outputs = 32\n",
    "    conv_ksize = (5,5)\n",
    "    conv_strides = (1,1) # tried (3,3) as well\n",
    "    pool_ksize = (2,2)\n",
    "    pool_strides = (2,2)\n",
    "    logits = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    conv_num_outputs = 64\n",
    "    conv_ksize = (5,5)\n",
    "    conv_strides = (1,1)\n",
    "    pool_ksize = (2,2)\n",
    "    pool_strides = (2,2)\n",
    "    logits = conv2d_maxpool(logits, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    conv_num_outputs = 128\n",
    "    conv_ksize = (5,5)\n",
    "    conv_strides = (1,1)\n",
    "    pool_ksize = (2,2)\n",
    "    pool_strides = (2,2)\n",
    "    logits = conv2d_maxpool(logits, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "    conv_num_outputs = 256\n",
    "    conv_ksize = (5,5)\n",
    "    conv_strides = (1,1) # earlier 3,3 \n",
    "    pool_ksize = (2,2)\n",
    "    pool_strides = (2,2)\n",
    "    logits = conv2d_maxpool(logits, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    # gave just 46 % with this layer included at dropout = 0.69 w/0 a third fcc layer\n",
    "    # increased  validation accuracy with droput at 0.6\n",
    "    tf.nn.dropout(logits, keep_prob)    \n",
    "    logits = flatten(logits)\n",
    "    \n",
    "    num_outputs = 512\n",
    "    logits = fully_conn(logits, num_outputs)\n",
    "    \n",
    "    num_outputs = 512\n",
    "    logits = fully_conn(logits, num_outputs)\n",
    "    \n",
    "    num_outputs = 512\n",
    "    logits = fully_conn(logits, num_outputs)\n",
    "    # not included initialy    \n",
    "    \n",
    "    num_outputs = 10\n",
    "    logits = output(logits, num_outputs)\n",
    "    \n",
    "    return logits\n",
    "\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "\n",
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    session.run(optimizer, feed_dict={\n",
    "                x: feature_batch,\n",
    "                y: label_batch,\n",
    "                keep_prob: keep_probability})\n",
    "    \n",
    "\n",
    "\n",
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    validation_loss = sess.run(cost, feed_dict={\n",
    "        x: valid_features,\n",
    "        y: valid_labels,\n",
    "        keep_prob: 1})\n",
    "    validation_accuracy = sess.run(accuracy, feed_dict={\n",
    "        x: valid_features,\n",
    "        y: valid_labels,\n",
    "        keep_prob: 1})\n",
    "    print(\"validation_loss: {}, validation_accuracy: {}\".format(validation_loss, validation_accuracy))\n",
    "\n",
    "# Tune Parameters\n",
    "epochs = 25 #trial and error\n",
    "batch_size = 128\n",
    "keep_probability = 0.50 #changed from 0.69 works great !\n",
    "\n",
    "# epochs = 35 #trial and error\n",
    "# batch_size = 256\n",
    "# keep_probability = 0.50 #change\n",
    "\n",
    "\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)\n",
    "\n",
    "#     try one to one convolution just for a try \n",
    "#     conv_num_outputs = 128\n",
    "#     conv_ksize = (1,1)\n",
    "#     conv_strides = (1,1)\n",
    "#     pool_ksize = (2,2)\n",
    "#     pool_strides = (2,2)\n",
    "#     logits = conv2d_maxpool(logits, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "#horribly bad    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\thanusha\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.5892714968152867\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-2-ef24d0d1c60c>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-ef24d0d1c60c>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    \"\"\"\u001b[0m\n\u001b[1;37m       \n^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "########\n",
    "def display_stats(cifar10_dataset_folder_path, batch_id, sample_id):\n",
    "\"\"\"\n",
    "#Display Stats of the the dataset\n",
    "\"\"\"\n",
    "batch_ids = list(range(1, 6))\n",
    "if batch_id not in batch_ids:\n",
    "print('Batch Id out of Range. Possible Batch Ids: {}'.format(batch_ids))\n",
    "return None\n",
    "features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_id)\n",
    "if not (0 <= sample_id < len(features)):\n",
    "print('{} samples in batch {}. {} is out of range.'.format(len(features)\n",
    "return None\n",
    "print('\\nStats of batch {}:'.format(batch_id))\n",
    "print('Samples: {}'.format(len(features)))\n",
    "print('Label Counts: {}'.format(dict(zip(*np.unique(labels, return_counts=True\n",
    "print('First 20 Labels: {}'.format(labels[:20]))\n",
    "sample_image = features[sample_id]\n",
    "sample_label = labels[sample_id]\n",
    "label_names = _load_label_names()\n",
    "print('\\nExample of Image {}:'.format(sample_id))\n",
    "print('Image - Min Value: {} Max Value: {}'.format(sample_image.min(), sample_\n",
    "print('Image - Shape: {}'.format(sample_image.shape))\n",
    "print('Label - Label Id: {} Name: {}'.format(sample_label, label_names[sample_\n",
    "plt.axis('off')\n",
    "                                                                       \n",
    "                                                                       plt.imshow(sample_image)\n",
    "\n",
    "def _preprocess_and_save(normalize, one_hot_encode, features, labels, filename):\n",
    "\n",
    "#Preprocess data and save it to file\n",
    "\n",
    "features = normalize(features)\n",
    "labels = one_hot_encode(labels)\n",
    "pickle.dump((features, labels), open(filename, 'wb'))\n",
    "def preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode):\n",
    "\n",
    "#Preprocess Training and Validation Data\n",
    "\n",
    "n_batches = 5\n",
    "valid_features = []\n",
    "valid_labels = []\n",
    "for batch_i in range(1, n_batches + 1):\n",
    "features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_i\n",
    "validation_count = int(len(features) * 0.1)\n",
    "# Prprocess and save a batch of training data\n",
    "def _preprocess_and_save(normalize,one_hot_encode,\n",
    "features[:-validation_count],\n",
    "labels[:-validation_count],\n",
    "'preprocess_batch_' + str(batch_i) + '.p')\n",
    "# Use a portion of training batch for validation\n",
    "valid_features.extend(features[-validation_count:])\n",
    "valid_labels.extend(labels[-validation_count:])\n",
    "# Preprocess and Save all validation data\n",
    "_preprocess_and_save(\n",
    "normalize,\n",
    "one_hot_encode,\n",
    "np.array(valid_features),\n",
    "np.array(valid_labels),\n",
    "'preprocess_validation.p')\n",
    "with open(cifar10_dataset_folder_path + '/test_batch', mode='rb') as file:\n",
    "batch = pickle.load(file, encoding='latin1')\n",
    "# load the training data\n",
    "test_features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpo\n",
    "test_labels = batch['labels']\n",
    "# Preprocess and Save all training data\n",
    "def _preprocess_and_save(\n",
    "normalize,\n",
    "one_hot_encode,\n",
    "np.array(test_features),\n",
    "\n",
    "Image_classification\n",
    "\n",
    "_preprocess_and_save(normalize, one_hot_encode, features, labels, filename):\n",
    "\n",
    "\n",
    "np.array(test_labels),\n",
    "'preprocess_training.p')\n",
    "def batch_features_labels(features, labels, batch_size):\n",
    "\"\"\"\n",
    "#Split features and labels into batches\n",
    "\"\"\"\n",
    "for start in range(0, len(features), batch_size):\n",
    "end = min(start + batch_size, len(features))\n",
    "yield features[start:end], labels[start:end]\n",
    "def load_preprocess_training_batch(batch_id, batch_size):\n",
    "\"\"\"\n",
    "#Load the Preprocessed Training data and return them in batches of <batch_size\n",
    "\"\"\"\n",
    "filename = 'preprocess_batch_' + str(batch_id) + '.p'\n",
    "features, labels = pickle.load(open(filename, mode='rb'))\n",
    "# Return the training data in batches of size <batch_size> or less\n",
    "return batch_features_labels(features, labels, batch_size)\n",
    "def display_image_predictions(features, labels, predictions):\n",
    "n_classes = 10\n",
    "label_names = _load_label_names()\n",
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(range(n_classes))\n",
    "label_ids = label_binarizer.inverse_transform(np.array(labels))\n",
    "fig, axies = plt.subplots(nrows=4, ncols=2)\n",
    "fig.tight_layout()\n",
    "fig.suptitle('Softmax Predictions', fontsize=20, y=1.1)\n",
    "n_predictions = 3\n",
    "margin = 0.05\n",
    "ind = np.arange(n_predictions)\n",
    "width = (1. - 2. * margin) / n_predictions\n",
    "for image_i, (feature, label_id, pred_indicies, pred_values) in enumerate(zip\n",
    "pred_names = [label_names[pred_i] for pred_i in pred_indicies]\n",
    "correct_name = label_names[label_id]\n",
    "axies[image_i][0].imshow(feature*255)\n",
    "axies[image_i][0].set_title(correct_name)\n",
    "axies[image_i][0].set_axis_off()\n",
    "axies[image_i][1].barh(ind + margin, pred_values[::-1], width)\n",
    "axies[image_i][1].set_yticks(ind + margin)\n",
    "axies[image_i][1].set_yticklabels(pred_names[::-1])\n",
    "axies[image_i][1].set_xticks([0, 0.5, 1.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
